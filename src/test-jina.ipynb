{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a7b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import hjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2e4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPU disponible. Usaremos CUDA y limitaremos a ~3800 MiB de VRAM.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "    print(f\"[INFO] GPU disponible. Usaremos CUDA y limitaremos a ~3800 MiB de VRAM.\")\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "    print(\"[WARN] GPU no disponible. Usando CPU sin limitación de memoria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6eb914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModel.from_pretrained(\n",
    "    \"../models/jina-reranker-m0\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={\n",
    "        0: \"3000MB\"\n",
    "    }\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(\n",
    "    \"../models/jina-reranker-m0\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2d282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/tables.hjson\", \"r\") as file:\n",
    "    tables=hjson.load(file)\n",
    "\n",
    "table_docs={\n",
    "    \"INVENTARIO_REPORTES\": f\"{tables['table_reports'][0]}\\n{tables['table_reports'][1]}\",\n",
    "    \"INVENTARIO_VALIDACIONES\": f\"{tables['table_valid'][0]}\\0{tables['table_valid'][1]}\",\n",
    "    \"INVENTARIO_VALIDACIONES_COMPUESTO\": f\"{tables['table_report_valid'][0]}\\n{tables['table_report_valid'][1]}\",\n",
    "    \"CLIENTE\": f\"{tables['table_client'][0]}\\n{tables['table_client'][1]}\",\n",
    "    \"CONTRATOS\": f\"{tables['table_contra'][0]}\\n{tables['table_contra'][1]}\",\n",
    "    \"CONTRATOS_REPORTES\": f\"{tables['table_contra_report'][0]}\\n{tables['table_contra_report'][1]}\",\n",
    "    \"CONTRATOS_CLIENTES_REPORTES\": f\"{tables['table_client_contra_report'][0]}\\n{tables['table_client_contra_report'][1]}\",\n",
    "    \"GENERALES\": f\"{tables['general_quest']}\",\n",
    "}\n",
    "table_desc={\n",
    "    \"INVENTARIO_REPORTES\": tables['table_reports'][1],\n",
    "    \"INVENTARIO_VALIDACIONES\": tables['table_valid'][1],\n",
    "    \"INVENTARIO_VALIDACIONES_COMPUESTO\": tables['table_report_valid'][1],\n",
    "    \"CLIENTE\": tables['table_client'][1],\n",
    "    \"CONTRATOS\": tables['table_contra'][1],\n",
    "    \"CONTRATOS_REPORTES\": tables['table_contra_report'][1],\n",
    "    \"CONTRATOS_CLIENTES_REPORTES\": tables['table_client_contra_report'][1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f51cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_table(query: str) -> str:\n",
    "    table_names = list(table_docs.keys())\n",
    "    pairs = [[query, schema] for schema in table_docs.values()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # scores quizá sea una lista de floats o un tensor en CPU\n",
    "        scores = model.compute_score(pairs, max_length=1024, doc_type=\"text\")\n",
    "    \n",
    "    # Si scores es un tensor:\n",
    "    if isinstance(scores, torch.Tensor):\n",
    "        scores = scores.cpu().tolist()\n",
    "\n",
    "    # Imprime cada nombre de tabla con su score\n",
    "    for name, score in zip(table_names, scores):\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "    # Ahora selecciona el índice del mayor score\n",
    "    best_idx = int(torch.argmax(torch.tensor(scores)))\n",
    "    choose=list(table_docs.keys())[best_idx]\n",
    "    print(f\"\\nMejor tabla: {choose} (score = {scores[best_idx]:.4f})\")\n",
    "\n",
    "    return [choose]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd27e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVENTARIO_REPORTES: 0.7903\n",
      "INVENTARIO_VALIDACIONES: 0.7643\n",
      "INVENTARIO_VALIDACIONES_COMPUESTO: 0.7292\n",
      "CLIENTE: 0.7712\n",
      "CONTRATOS: 0.7607\n",
      "CONTRATOS_REPORTES: 0.7535\n",
      "CONTRATOS_CLIENTES_REPORTES: 0.7292\n",
      "GENERALES: 0.8191\n",
      "\n",
      "Mejor tabla: GENERALES (score = 0.8191)\n",
      "→ “Cuales son ?”  se refiere más a: ['GENERALES']\n"
     ]
    }
   ],
   "source": [
    "ejemplos=[\n",
    "    \"Cuales son ?\",\n",
    "]\n",
    "for q in ejemplos:\n",
    "    tabla=choose_table(q)\n",
    "    print(f\"→ “{q}”  se refiere más a: {tabla}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
